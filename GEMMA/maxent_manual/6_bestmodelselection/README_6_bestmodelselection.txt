maxENT modelling:

To find the best model out of each replicate set: the scripts used are present in the replciate folder (/netscratch/dep_coupland/grp_fulgione/siva/final_maxent/replicates/rep_1/maxent1.R) named as maxent1.R and the best model is stored in the Calibration_results folder, this maxent1.R script will calculate the "Mean_AUC_ratio","pval_pROC","Omission_rate_at_5%","AICc","delta_AICc","W_AICc" for each model with different parameter 

Now for each replicate, we have 70 models performance metrics, thus we have 700 models where 10 are replicates of each other, thus we complied all the calibration_results.csv from (/netscratch/dep_coupland/grp_fulgione/siva/final_maxent/replicates/rep_$i/Calibration_results for individal i ranging from 1 to 10) into the file named as modified_10.csv in 
(/netscratch/dep_coupland/grp_fulgione/siva/final_maxent/final_model/bestmodel)

then to find the best model out of them all, we created the summary_stats_10.csv file by calculating the mean_AUC_median, AICC_median, delta_AICc_median, mean_omission_rate_median from the 10 replicates for each model, then out of them all, we looked into the model parameter with highest mean AUC median value and lowest Aicc median and omission rate, thus we chose the model with regularization value of 0.1 and feature class of linear and product (lp)

Best model is named as (M_0.1_F_lp) , the model information is present within the folder named (M_0.1_F_lp_Best_variables)

Information on the model evaluation is described in below detail:

Model evaluation and best model selection: We used Maxent (3.4.1), implemented using the R package kuenm (Phillips et al., 2006; Cobos et al., 2019b), to calibrate parameter values, evaluate candidate models, and make future projections.  We ran ten different replicate training/test splits of 70 models (7 feature classes x  10 regularization multipliers). The best model among the 70 was identified by looking at the optimal parameter configurations assessed using multiple statistics that measure model performance. (Sutton & Martin, 2022). The metrics include the Mean AUC test (Area under the curve), which evaluates the model's discriminatory performance between predicted presence in withheld test data and pseudo-absence points. An AUC below 0.8 indicates a poor model, 0.8 to 0.9 is considered fair, 0.9 to 0.995 is good, and above 0.995 is excellent (Fielding & Bell, 1997). Higher AUC test values signify improved discrimination between testing and background points. Secondly, the corrected Akaike Information Criterion for small sample sizes (AICc) is a model selection that indicates how well the model fits the data with the model's complexity. Calculated using the entire species occurrence dataset, AICc helps select optimal parameter configurations by favoring models with lower AICc values. Lower AICc (close to 0) values signify better-performing models, considering both fit to the data and model simplicity (Kass et al., 2014). delta AICc is calculated by finding the difference between each modelâ€™s AICc and the minimum AICc among the whole trained set of models. Lastly, the Mean Omission rate refers to the proportion of known species occurrences that the model fails to predict correctly, and a 10% training omission rate is chosen to identify overfit models with omission rates exceeding the expected 0.10 threshold (Boria et al., 2014). Models showing progressively higher omission rates than expected are considered more overfit (Boria et al., 2017).


